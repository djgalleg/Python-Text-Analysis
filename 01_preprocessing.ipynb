{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d3e7ea21-6437-48e8-a9e4-3bdc05f709c9","cell_type":"markdown","source":"# Analisis de texto en Python: Preprocesamiento\n\n* * * \n\n<div class=\"alert alert-success\">  \n    \n### Objetivos de aprendizaje \n    \n* Aprender pasos comunes para preprocesamiento de datos de texto, tan bien como especificar operaciones para preprocesamiento de datos de Twitter.\n* Conocer los paquetes NPL comunmente usado y acoplarlos.\n* Entender tokenizadores, y como cambiar desde la llegada de los grandes modelos de lenguaje.\n</div>\n\n### Icons Used in This Notebook\nğŸ”” **Pregunta**: Una pregunta rapida para ayudar a entender que esta sucediendo.<br>\nğŸ¥Š **Reto**: Ejercicios interactivos. Trabajaremos a traves de esto en el taller!<br>\nâš ï¸ **Advertencia:** Aviso sobre cosas complicadas o errores comunes.<br>\nğŸ¬ **Demo**: Mostrando algo mÃ¡s avanzado: para que sepas para quÃ© se puede usar Python!<br> \n\n### Secciones\n1. [Preprocessing](#section1)\n2. [Tokenization](#section2)\n\nEn esta seccion de tres parte del taller, aprenderemos la construccion de bloques para ejecutar anlisis de texto en Python. Estas tecnicas se encuentran en el dominio de Procesamiento de Lenguaje Natural (NLP). NLP es un campo que se ocupa con identificacion y extrayendo patrones de lenguaje, ante todo escribiendo textos. A lo largo de la serie de talleres, interactuaremos con paquetes para ejecutar analisis de texto: empezando desde metodos de cadena simple hasta paquetes especificos  NLP, tal como `nltk`, `spaCy`, y mas recientes sobre Modelos de Lenguaje Grande (`BERT`).\n\nAhora, Instalemos estos paquetes correctamente ante de introducirnos en la materia.","metadata":{}},{"id":"d442e4c7-e926-493d-a64e-516616ad915a","cell_type":"code","source":"# Uncomment the following lines to install packages/model\n%pip install NLTK\n%pip install transformers\n%pip install spaCy\n!python -m spacy download en_core_web_sm","metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting NLTK\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting click (from NLTK)\n  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\nCollecting joblib (from NLTK)\n  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting regex>=2021.8.3 (from NLTK)\n  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\nCollecting tqdm (from NLTK)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\nDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\nDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nInstalling collected packages: tqdm, regex, joblib, click, NLTK\nSuccessfully installed NLTK-3.9.1 click-8.1.8 joblib-1.4.2 regex-2024.11.6 tqdm-4.67.1\nNote: you may need to restart the kernel to use updated packages.\nCollecting transformers\n  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\nCollecting filelock (from transformers)\n  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\nCollecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\nCollecting numpy>=1.17 (from transformers)\n  Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\nRequirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /srv/conda/envs/notebook/lib/python3.10/site-packages (from transformers) (2.32.3)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting safetensors>=0.4.3 (from transformers)\n  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nRequirement already satisfied: tqdm>=4.27 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from transformers) (4.67.1)\nCollecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.26.0->transformers)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\nDownloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\nDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\nDownloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\nDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\nDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m135.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\nInstalling collected packages: safetensors, numpy, fsspec, filelock, huggingface-hub, tokenizers, transformers\nSuccessfully installed filelock-3.18.0 fsspec-2025.3.0 huggingface-hub-0.29.3 numpy-2.2.4 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.0\nNote: you may need to restart the kernel to use updated packages.\nCollecting spaCy\n  Downloading spacy-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\nCollecting spacy-legacy<3.1.0,>=3.0.11 (from spaCy)\n  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\nCollecting spacy-loggers<2.0.0,>=1.0.0 (from spaCy)\n  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\nCollecting murmurhash<1.1.0,>=0.28.0 (from spaCy)\n  Downloading murmurhash-1.0.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\nCollecting cymem<2.1.0,>=2.0.2 (from spaCy)\n  Downloading cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\nCollecting preshed<3.1.0,>=3.0.2 (from spaCy)\n  Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\nCollecting thinc<8.4.0,>=8.3.4 (from spaCy)\n  Downloading thinc-8.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\nCollecting wasabi<1.2.0,>=0.9.1 (from spaCy)\n  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\nCollecting srsly<3.0.0,>=2.4.3 (from spaCy)\n  Downloading srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\nCollecting catalogue<2.1.0,>=2.0.6 (from spaCy)\n  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\nCollecting weasel<0.5.0,>=0.1.0 (from spaCy)\n  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\nCollecting typer<1.0.0,>=0.3.0 (from spaCy)\n  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from spaCy) (4.67.1)\nRequirement already satisfied: numpy>=1.19.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from spaCy) (2.2.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from spaCy) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from spaCy) (2.0.3)\nRequirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from spaCy) (3.1.5)\nRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.10/site-packages (from spaCy) (75.8.0)\nRequirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from spaCy) (24.2)\nCollecting langcodes<4.0.0,>=3.2.0 (from spaCy)\n  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\nCollecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spaCy)\n  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.3.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (2.3.0)\nRequirement already satisfied: typing-extensions>=4.6.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (4.12.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2024.12.14)\nCollecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spaCy)\n  Downloading blis-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nCollecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spaCy)\n  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: click>=8.0.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spaCy) (8.1.8)\nCollecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spaCy)\n  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\nCollecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spaCy)\n  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\nCollecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spaCy)\n  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\nCollecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spaCy)\n  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from jinja2->spaCy) (3.0.2)\nCollecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spaCy)\n  Downloading marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\nCollecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy)\n  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (2.19.1)\nCollecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spaCy)\n  Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\nCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy)\n  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\nDownloading spacy-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.2/29.2 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\nDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\nDownloading cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (204 kB)\nDownloading langcodes-3.5.0-py3-none-any.whl (182 kB)\nDownloading murmurhash-1.0.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\nDownloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\nDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\nDownloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\nDownloading srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\nDownloading thinc-8.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m126.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading typer-0.15.2-py3-none-any.whl (45 kB)\nDownloading wasabi-1.1.3-py3-none-any.whl (27 kB)\nDownloading weasel-0.4.1-py3-none-any.whl (50 kB)\nDownloading blis-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\nDownloading confection-0.1.5-py3-none-any.whl (35 kB)\nDownloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m160.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\nDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\nDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\nDownloading marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\nDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\nDownloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\nDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nInstalling collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, blis, srsly, smart-open, preshed, markdown-it-py, language-data, rich, langcodes, confection, typer, thinc, weasel, spaCy\nSuccessfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 preshed-3.0.9 rich-13.9.4 shellingham-1.5.4 smart-open-7.1.0 spaCy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\nNote: you may need to restart the kernel to use updated packages.\nCollecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-3.8.0\n\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n"}],"execution_count":1},{"id":"df5b8f8e-4e69-426e-a202-ec48b325e89a","cell_type":"markdown","source":"<a id='section1'></a>\n\n# Preprocesamiento\n\nEn la parte 1 de este taller, Direccionaremos el primer paso de analisis de texto. Nuestra meta es convertir la fila, datos de texto desordenados en un formato consistente. Este proceso es a menudo llamado **preprocesamiento**, **limpieza de texto**, o **normalizaciÃ³n de texto**.\n\nNotaras que al final del procesamiento,nuestro sto es aun en formato que nosostros podemos leer y entender. En la parte 2 y 3, comenzaremos nuestra incursiÃ³n en la conversiÃ³n de datos de texto en una representaciÃ³n numÃ©rica, un formato que las computadoras pueden manejar mÃ¡s fÃ¡cilmente. \n\nğŸ”” **Pregunta**: Vamos a pausar un momento para reflexionar sobre **sus** previas experiencias trabajando sobre texto de datos. \n- Cual es el formato de los datos de texto con los que has interactuado (plain text, CSV, or XML)?\n- De donde viene (structured corpus, scraped from the web, survey data)?\n- Esta desordenado (i.e., is the data formatted consistently)?","metadata":{}},{"id":"4b35911a-3b3f-4a48-a7d1-9882aab04851","cell_type":"markdown","source":"## Procesos Comunes\n\nPreprocesamiento no es aldo que podamos lograr con una simple linea de cÃ³digo. Nosotros a menudo empezamos por familiarizarnos nosotros mismo con los datos, y en el camino, obtenemos una comprensiÃ³n mÃ¡s clara de la granularidad del preprocesamiento que queremos aplicar.\n\nInicialmente, comenzamos aplicando un conjunto de procesos comÃºnmente utilizados para limpiar los datos. Estas operaciones so alteran facilmente la forma o el significado de los datos; sirven como un procedimiento estandarizado para remodelar los datos en un formato consistente.\n\nLa siguientes procesos, por ejemplo, son comunmente aplicados para procesos de textos en ingles de varios generos. Estas operaciones pueden  estar siendo usadas para funciones integrles en Python, tal como metodos`string`, y expresiones regulares. \n- El texto en minuscula\n- Remover puntuaciones marcadas \n- Remover caracteres de espacio en blanco\n- Remover palabras en stop \n\nDespuÃ©s al iniciar el procesamiento, nosotros debemos cambiar para realizar procesos de tareas especificas, cuyos detalles a menudo dependen de la tarea posterior que queremos realizar y de la naturaleza de los datos de texto (i.e., its stylistic and linguistic features).  \n\nAntes de adentrarnos en estas operaciones, echemos un vistazo a nuestros datos!\n","metadata":{}},{"id":"ec5d7350-9a1e-4db9-b828-a87fe1676d8d","cell_type":"markdown","source":"### Importar el texto de datos\n\nEl texto de datos, podemos estar trabajando con un archivo CSV. Contiene tuits sobre aerolÃ­neas estadounidenses, eliminados desde febrero de 2015. \n\nVamos a leer el archivo `airline_tweets.csv` dentro del dataframe con `pandas`.","metadata":{}},{"id":"7228677e-001b-4484-b85d-9d4218a1469d","cell_type":"code","source":"!pip show pandas","metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":"\u001b[33mWARNING: Package(s) not found: pandas\u001b[0m\u001b[33m\n\u001b[0m"}],"execution_count":1},{"id":"f912f8a1-662c-4e66-88ad-36460c6db98c","cell_type":"code","source":"!pip install pandas","metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting pandas\n  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\nRequirement already satisfied: numpy>=1.22.4 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2.2.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2025.1)\nCollecting tzdata>=2022.7 (from pandas)\n  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\nInstalling collected packages: tzdata, pandas\nSuccessfully installed pandas-2.2.3 tzdata-2025.2\n"}],"execution_count":2},{"id":"3d1ff64b-53ad-4eca-b846-3fda20085c43","cell_type":"code","source":"# Import pandas\nimport pandas as pd\n\n# File path to data\ncsv_path = '../data/airline_tweets.csv'\n\n# Specify the separator\ntweets = pd.read_csv(csv_path, sep=',')","metadata":{"trusted":false},"outputs":[],"execution_count":3},{"id":"e397ac6a-c2ba-4cce-8700-b36b38026c9d","cell_type":"code","source":"# Show the first five rows\ntweets.head()","metadata":{"trusted":false},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>airline_sentiment</th>\n      <th>airline_sentiment_confidence</th>\n      <th>negativereason</th>\n      <th>negativereason_confidence</th>\n      <th>airline</th>\n      <th>airline_sentiment_gold</th>\n      <th>name</th>\n      <th>negativereason_gold</th>\n      <th>retweet_count</th>\n      <th>text</th>\n      <th>tweet_coord</th>\n      <th>tweet_created</th>\n      <th>tweet_location</th>\n      <th>user_timezone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>570306133677760513</td>\n      <td>neutral</td>\n      <td>1.0000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>cairdin</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica What @dhepburn said.</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:35:52 -0800</td>\n      <td>NaN</td>\n      <td>Eastern Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>570301130888122368</td>\n      <td>positive</td>\n      <td>0.3486</td>\n      <td>NaN</td>\n      <td>0.0000</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica plus you've added commercials t...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:59 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>570301083672813571</td>\n      <td>neutral</td>\n      <td>0.6837</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>yvonnalynn</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:48 -0800</td>\n      <td>Lets Play</td>\n      <td>Central Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>570301031407624196</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Bad Flight</td>\n      <td>0.7033</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica it's really aggressive to blast...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:36 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>570300817074462722</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Can't Tell</td>\n      <td>1.0000</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica and it's a really big bad thing...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:14:45 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n0  570306133677760513           neutral                        1.0000   \n1  570301130888122368          positive                        0.3486   \n2  570301083672813571           neutral                        0.6837   \n3  570301031407624196          negative                        1.0000   \n4  570300817074462722          negative                        1.0000   \n\n  negativereason  negativereason_confidence         airline  \\\n0            NaN                        NaN  Virgin America   \n1            NaN                     0.0000  Virgin America   \n2            NaN                        NaN  Virgin America   \n3     Bad Flight                     0.7033  Virgin America   \n4     Can't Tell                     1.0000  Virgin America   \n\n  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n0                    NaN     cairdin                 NaN              0   \n1                    NaN    jnardino                 NaN              0   \n2                    NaN  yvonnalynn                 NaN              0   \n3                    NaN    jnardino                 NaN              0   \n4                    NaN    jnardino                 NaN              0   \n\n                                                text tweet_coord  \\\n0                @VirginAmerica What @dhepburn said.         NaN   \n1  @VirginAmerica plus you've added commercials t...         NaN   \n2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n3  @VirginAmerica it's really aggressive to blast...         NaN   \n4  @VirginAmerica and it's a really big bad thing...         NaN   \n\n               tweet_created tweet_location               user_timezone  \n0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"execution_count":4},{"id":"ae3b339f-45cf-465d-931c-05f9096fd510","cell_type":"markdown","source":"El marco de datos tiene una fila por tuit. El texto del tuit se muestra en la columna `text`.\n- `text` (`str`): el texto del tweet.\n\nOtros metadatos que nos interesan incluyen: \n- `airline_sentiment` (`str`): the sentiment of the tweet, etiquetado como as \"neutral,\" \"positive,\" o \"negative.\"\n- `airline` (`str`): the airline that is tweeted about.\n- `retweet count` (`int`): como algunos tiempo el tweet fueron retweeteados.","metadata":{}},{"id":"302c695b-4bd1-4151-9cb9-ef5253eb16df","cell_type":"markdown","source":"Echemos un vistazo a algunos de los tweets:","metadata":{}},{"id":"b690daab-7be5-4b8f-8af0-a91fdec4ec4f","cell_type":"code","source":"!pip install pandas\n# Ejemplo de creaciÃ³n de un DataFrame llamado 'tweets'\nimport pandas as pd\ntweets = pd.DataFrame({\n    'text': ['tweet 1', 'tweet 2', 'tweet 3']\n})\nprint(tweets['text'].iloc[0])\nprint(tweets['text'].iloc[1])\nprint(tweets['text'].iloc[2])\nprint(tweets.head())","metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.10/site-packages (2.2.3)\nRequirement already satisfied: numpy>=1.22.4 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2.2.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2025.2)\nRequirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\ntweet 1\ntweet 2\ntweet 3\n      text\n0  tweet 1\n1  tweet 2\n2  tweet 3\n"}],"execution_count":7},{"id":"8adc05fa-ad30-4402-ab56-086bcb09a166","cell_type":"markdown","source":"ğŸ”” **Pregunta**: Que has notado? CuÃ¡les son las caracterÃ­sticas estilÃ­sticas de los tweets?","metadata":{}},{"id":"c3460393-00a6-461c-b02a-9e98f9b5d1af","cell_type":"markdown","source":"### Lowercasing\n\nMientras reconocasmos que el uso de mayÃºsculas y minÃºsculas de una palabra es informativo, a menudo no trabajamos en contextos en los que podamos utilizar adecuadamente esta informaciÃ³n.\n\nMas a menudo, el anÃ¡lisis posterior nosotros realizamos **case-insensitive**. Por ejemplo, en el anÃ¡lisis de frecuencia, queremos tener en cuenta las diversas formas de una misma palabra. Convertir los datos de texto en minÃºsculas facilita este proceso y simplifica nuestro anÃ¡lisis.\n\nPodemos lograr fÃ¡cilmente la conversiÃ³n a minÃºsculas con el mÃ©todo de cadena [`.lower()`](https://docs.python.org/3/library/stdtypes.html#str.lower); see [documentation](https://docs.python.org/3/library/stdtypes.html#string-methods) para mas funciones Ãºtiles.\n\nVamos aplicar esto en el sigiente ejemplo:","metadata":{}},{"id":"58a95d90-3ef1-4bff-9cfe-d447ed99f252","cell_type":"code","source":"# Verificar cuÃ¡ntas filas tiene el DataFrame / se cambia el cÃ³digo ya que no se define bien \nprint(\"NÃºmero de filas en el DataFrame:\", tweets.shape[0])\n\n# Verificar si el Ã­ndice 108 estÃ¡ dentro del rango de las filas\nif tweets.shape[0] > 108:\n    first_example = tweets['text'].iloc[108]  # Accede al tweet en el Ã­ndice 108\n    print(first_example)\nelse:\n    print(\"Ãndice 108 fuera de rango. El DataFrame solo tiene\", tweets.shape[0], \"filas.\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"NÃºmero de filas en el DataFrame: 3\nÃndice 108 fuera de rango. El DataFrame solo tiene 3 filas.\n"}],"execution_count":17},{"id":"c66d91c0-6eed-4591-95fc-cd2eae2e0d41","cell_type":"code","source":"# Ejemplo: asignar un valor a 'first_example'\nfirst_example = \"Este es un ejemplo de texto\"\n\n# Dado que el codigo anterior no ejecuta ahora el cÃ³digo esta sin errores\n\n# Revisar si todos los caracteres estÃ¡n en minÃºscula \nprint(first_example.islower())  # Devuelve True si todos los caracteres son minÃºsculas\nprint(f\"{'=' * 50}\")\n\n# Convertirlo a minÃºsculas\nprint(first_example.lower())  # Convierte a minÃºsculas\nprint(f\"{'=' * 50}\")\n\n# Convertirlo a mayÃºsculas\nprint(first_example.upper())  # Convierte a mayÃºsculas","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"False\n==================================================\neste es un ejemplo de texto\n==================================================\nESTE ES UN EJEMPLO DE TEXTO\n"}],"execution_count":20},{"id":"7bf0d8c8-bd6c-47ef-b305-09ac61d07d4d","cell_type":"markdown","source":"### Eliminar caracteres de espacio en blanco adicionales\n\nA veces nosotros podriamos encontrar textos con espacios en blanco extra, such as spaces, tabs, and newline characters, which is particularly common when the text is scrapped from web pages. Before we dive into the details, let's briefly introduce Regular Expressions (regex) and the `re` package. \n\nLas expresiones regulares son una forma eficaz de buscar patrones de cadenas especÃ­ficos en corpus grandes. Su curva de aprendizaje es notablemente pronunciada, pero pueden ser muy eficientes una vez que las dominamos. Muchos paquetes de PNL dependen en gran medida de las expresiones regulares internamente. Los evaluadores de expresiones regulares, como [regex101](https://regex101.com), son herramientas utiles are useful tools tanto en la comprensiÃ³n como en la creaciÃ³n de expresiones regulares.\n\nNuestro objetivo en este taller no es proporcionar una inmersiÃ³n profunda (ni siquiera superficial) en las expresiones regulares; en cambio, queremos exponerlo a ellas para que estÃ© mejor preparado para realizar inmersiones profundas en el futuro.\n\nEl siguiente ejemplo es un poema de William Wordsworth. Como muchos poemas, el texto puede contener saltos de lÃ­nea adicionales. (i.e., newline characters, `\\n`) que queremos eliminar.","metadata":{}},{"id":"d1bd73f1-a30f-4269-a05e-47cfff7b496f","cell_type":"code","source":"# File path to the poem\ntext_path = '../data/poem_wordsworth.txt'\n\n# Read the poem in\nwith open(text_path, 'r') as file:\n    text = file.read()\n    file.close()","metadata":{"trusted":true},"outputs":[],"execution_count":1},{"id":"7a693dd9-9706-40b3-863f-f568020245f7","cell_type":"markdown","source":"Como puedes ver, el poema es fomateado como ua cedena continua de textos con los saltos de lÃ­nea se colocan al final de cada lÃ­nea, lo que dificulta la lectura. ","metadata":{}},{"id":"7e78a75a-8e15-4bcb-a416-783aa7f60ef3","cell_type":"code","source":"text","metadata":{"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"\"I wandered lonely as a cloud\\n\\n\\nI wandered lonely as a cloud\\nThat floats on high o'er vales and hills,\\nWhen all at once I saw a crowd,\\nA host, of golden daffodils;\\nBeside the lake, beneath the trees,\\nFluttering and dancing in the breeze.\\n\\nContinuous as the stars that shine\\nAnd twinkle on the milky way,\\nThey stretched in never-ending line\\nAlong the margin of a bay:\\nTen thousand saw I at a glance,\\nTossing their heads in sprightly dance.\\n\\nThe waves beside them danced; but they\\nOut-did the sparkling waves in glee:\\nA poet could not but be gay,\\nIn such a jocund company:\\nI gazedâ€”and gazedâ€”but little thought\\nWhat wealth the show to me had brought:\\n\\nFor oft, when on my couch I lie\\nIn vacant or in pensive mood,\\nThey flash upon that inward eye\\nWhich is the bliss of solitude;\\nAnd then my heart with pleasure fills,\\nAnd dances with the daffodils.\""},"metadata":{}}],"execution_count":3},{"id":"47cce993-c315-4aaa-87fe-149de8607f65","cell_type":"markdown","source":"Una funciÃ³n Ãºtil que podemos utilizar para mostrar el poema correctamente es `.splitlines()`. Como sugiere el nombre, divide una secuencia de texto larga en una lista de lÃ­neas siempre que haya un carÃ¡cter de nueva lÃ­nea.   ","metadata":{}},{"id":"ddeade7a-065d-49e6-bdd3-87a8ea8f6e6e","cell_type":"code","source":"# Dividir la cadena Ãºnica en una lista de lÃ­neas\ntext.splitlines()","metadata":{"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['I wandered lonely as a cloud',\n '',\n '',\n 'I wandered lonely as a cloud',\n \"That floats on high o'er vales and hills,\",\n 'When all at once I saw a crowd,',\n 'A host, of golden daffodils;',\n 'Beside the lake, beneath the trees,',\n 'Fluttering and dancing in the breeze.',\n '',\n 'Continuous as the stars that shine',\n 'And twinkle on the milky way,',\n 'They stretched in never-ending line',\n 'Along the margin of a bay:',\n 'Ten thousand saw I at a glance,',\n 'Tossing their heads in sprightly dance.',\n '',\n 'The waves beside them danced; but they',\n 'Out-did the sparkling waves in glee:',\n 'A poet could not but be gay,',\n 'In such a jocund company:',\n 'I gazedâ€”and gazedâ€”but little thought',\n 'What wealth the show to me had brought:',\n '',\n 'For oft, when on my couch I lie',\n 'In vacant or in pensive mood,',\n 'They flash upon that inward eye',\n 'Which is the bliss of solitude;',\n 'And then my heart with pleasure fills,',\n 'And dances with the daffodils.']"},"metadata":{}}],"execution_count":5},{"id":"44d3825b-0857-44e1-bf6a-d8c7a9032704","cell_type":"markdown","source":"Vamos a retornar a nuestros datos tweet para un ejemplo.","metadata":{}},{"id":"53a81ea9-65c4-474a-8530-35393555d1be","cell_type":"code","source":"# Imprimir el segundo ejemplo \n#second_example = tweets['text'][5]\n#second_example\n!pip install pandas\nimport pandas as pd\nimport os\n\nimport pandas as pd\nimport os\n\n# Verificar el directorio actual de trabajo\nprint(f\"Directorio actual de trabajo: {os.getcwd()}\")\n\n# Leer el archivo de texto como lÃ­neas simples\nfile_path = '../data/poem_wordsworth.txt'  \n\n# Leer el archivo lÃ­nea por lÃ­nea\nwith open(file_path, 'r') as file:\n    lines = file.readlines()\n\n# Imprimir las primeras 5 lÃ­neas del archivo para verificar su contenido\nprint(\"Primeras 5 lÃ­neas del archivo:\")\nfor i in range(min(5, len(lines))):  # Limitar a 5 lÃ­neas\n    print(lines[i])\n\ntweets = pd.DataFrame(lines, columns=['text'])\n\n# Verificar el contenido cargado\nprint(\"\\nPrimeras 5 filas del DataFrame:\")\nprint(tweets.head())\n\n# Ahora, si quieres acceder a un ejemplo especÃ­fico del DataFrame:\nsecond_example = tweets['text'].iloc[5]  # Acceder al segundo ejemplo\nprint(\"\\nSegundo ejemplo del DataFrame:\")\nprint(second_example)\n\n# Si deseas realizar otras operaciones en los datos, puedes hacerlo ahora, por ejemplo, convertir el texto a minÃºsculas:\nprint(\"\\nTexto en minÃºsculas del segundo ejemplo:\")\nprint(second_example.lower())","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.10/site-packages (2.2.3)\nRequirement already satisfied: numpy>=1.22.4 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2.2.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2025.2)\nRequirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nDirectorio actual de trabajo: /home/jovyan/lessons\nPrimeras 5 lÃ­neas del archivo:\nI wandered lonely as a cloud\n\n\n\n\n\nI wandered lonely as a cloud\n\nThat floats on high o'er vales and hills,\n\n\nPrimeras 5 filas del DataFrame:\n                                          text\n0               I wandered lonely as a cloud\\n\n1                                           \\n\n2                                           \\n\n3               I wandered lonely as a cloud\\n\n4  That floats on high o'er vales and hills,\\n\n\nSegundo ejemplo del DataFrame:\nWhen all at once I saw a crowd,\n\n\nTexto en minÃºsculas del segundo ejemplo:\nwhen all at once i saw a crowd,\n\n"}],"execution_count":16},{"id":"aef55865-36fd-4c06-a765-530cf3b53096","cell_type":"markdown","source":"En este caso, no queremos dividir el tweet en una lista de cadenas. Seguimos esperando una sola cadena de texto, pero queremos eliminar por completo el salto de lÃ­nea.\n\nEl mÃ©todo string `.strip()` elimina eficazmente los espacios en ambos extremos del texto. Sin embargo, no funcionarÃ¡ en nuestro ejemplo, ya que el carÃ¡cter de nueva lÃ­nea estÃ¡ en el medio de el string.","metadata":{}},{"id":"b933503b-4370-4dc4-b287-6dc2f9cdb1d4","cell_type":"code","source":"# Solo elimina los espacios en blanco en ambos extremos\nsecond_example.strip()","metadata":{"trusted":true},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'When all at once I saw a crowd,'"},"metadata":{}}],"execution_count":17},{"id":"b99b80b4-804f-460f-a2d5-adbd654902b3","cell_type":"markdown","source":"Este es donde la nube regex es realmete Ãºtil.","metadata":{}},{"id":"ceac9714-7053-4b2e-affb-71f8c3d2dcd9","cell_type":"code","source":"import re","metadata":{"trusted":true},"outputs":[],"execution_count":18},{"id":"d5f08d20-ba81-4e48-9e2a-5728148005b3","cell_type":"markdown","source":"Ahora, con regex, bÃ¡sicamente, la llamamos para que coincida con un patrÃ³n identificado en los datos de texto, y queremos realizar algunas operaciones con la parte coincidente: extraerla, reemplazarla con otra cosa o eliminarla por completo. Por lo tanto, el funcionamiento de las expresiones regulares se puede resumir en los siguientes pasos:\n\n- Identificar y escribir los patrones en regex (`r'PATTERN'`)\n- Escribir el remplazo de los patrones(`'REPLACEMENT'`)\n- Llamar la funciÃ³n especÃ­fica regex  (e.g., `re.sub()`)\n\nEn nuestro ejemplo, el patron que estamos buscando es `\\s`, cuÃ¡l es el nombre corto en expresiÃ³n regular para cualquier carÃ¡cter de espacio en blanco (`\\n` and `\\t` included). TambiÃ©n aÃ±adimos un cuantificador `+` en el final: `\\s+`. Significa que nos gustarÃ­a capturar una o mÃ¡s ocurrencias del carÃ¡cter de espacio en blanco.","metadata":{}},{"id":"1248d227-1149-4014-94a5-c05592a27a7e","cell_type":"code","source":"# Escribir un patron en regex\nblankspace_pattern = r'\\s+'","metadata":{"trusted":true},"outputs":[],"execution_count":1},{"id":"cc075c2e-1a1d-4393-a3ea-8ad7c118364b","cell_type":"markdown","source":"El reemplazo de uno o mÃ¡s espacios en blanco es exactamente un solo espacio, que es el lÃ­mite canÃ³nico de palabras en inglÃ©s. Cualquier espacio adicional se reducirÃ¡ a un solo espacio. ","metadata":{}},{"id":"c55cb2f1-f4ca-4b79-900c-f65ec303ddac","cell_type":"code","source":"# Ecribir un remplazo para identificaciÃ³n de patrones \nblankspace_repl = ' '","metadata":{"trusted":true},"outputs":[],"execution_count":2},{"id":"bc12e3d1-728a-429b-9c83-4dcc88590bc4","cell_type":"markdown","source":"Por Ãºltimo, pongamos todo junto usando la funciÃ³n [`re.sub()`](https://docs.python.org/3.11/library/re.html#re.sub), Lo que significa que queremos sustituir un patrÃ³n por un reemplazo. La funciÃ³n acepta tres argumentos: el patrÃ³n, el reemplazo y la cadena a la que queremos aplicar la funciÃ³n.","metadata":{}},{"id":"5249b24b-7111-4569-be29-c40efa5e148e","cell_type":"code","source":"# Remplazar los espacios en blanco(s) con ' '\n#clean_text = re.sub(pattern = blankspace_pattern, \n                    #repl = blankspace_repl, \n                    #string = second_example)\nimport re  # AsegÃºrate de importar el mÃ³dulo 're'\n\n# Definir el patrÃ³n para los espacios en blanco y el reemplazo\nblankspace_pattern = r'\\s+'  # Esto busca uno o mÃ¡s espacios en blanco\nblankspace_repl = ' '  # Esto reemplaza con un solo espacio\n\n# Suponiendo que 'second_example' es el texto que quieres limpiar\nsecond_example = \"Este es    un  ejemplo   con    espacios   extras.\"\n\n# Remplazar los espacios en blanco(s) con ' '\nclean_text = re.sub(pattern=blankspace_pattern, \n                    repl=blankspace_repl, \n                    string=second_example)\n\nprint(clean_text)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Este es un ejemplo con espacios extras.\n"}],"execution_count":5},{"id":"a895fbe3-a034-4124-94af-72a528913c51","cell_type":"markdown","source":"Ta-da! El carÃ¡cter de nueva lÃ­nea ya no estÃ¡ allÃ­.","metadata":{}},{"id":"7087dc0c-5fef-4f1c-8662-7cbc8a978f34","cell_type":"markdown","source":"### Eliminar puntuaciones marcadas \n\nA veces sÃ³lo nos interesa analizar **alphanumeric characters** (i.e., the letters and numbers), en tal caso podrÃ­amos querer eliminar los signos de puntuaciÃ³n. \n\nEl modulo `string` contiene una lista predefinida de puntuaciones marcadas predefinidas. Vamos a imprimir esto.","metadata":{}},{"id":"70e8502b-b703-45e0-8852-0c3210363440","cell_type":"code","source":"# Cargar una lista predefinida de signos de puntuaciÃ³n\nfrom string import punctuation\nprint(punctuation)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"}],"execution_count":6},{"id":"91119c9e-431c-42cb-afea-f7e607698929","cell_type":"markdown","source":"En la prÃ¡ctica, para eliminar estos caracteres de puntuaciÃ³n, podemos simplemente iterar sobre el texto y eliminar los caracteres que se encuentran en la lista, como se muestra una funciÃ³n a continuaciÃ³n `remove_punct`.","metadata":{}},{"id":"237d868d-339d-4bbe-9a3b-20fa5fbdf231","cell_type":"code","source":"def remove_punct(text):\n    '''Remove punctuation marks in input text'''\n    \n    # Select characters not in puncutaion\n    no_punct = []\n    for char in text:\n        if char not in punctuation:\n            no_punct.append(char)\n\n    # Join the characters into a string\n    text_no_punct = ''.join(no_punct)   \n    \n    return text_no_punct","metadata":{"trusted":true},"outputs":[],"execution_count":7},{"id":"d4fc768b-c2dd-4386-8212-483c4485e4be","cell_type":"markdown","source":"Vamos aplicar la funciÃ³n del ejemplo below. ","metadata":{}},{"id":"7596c465-3d85-4b72-a853-f2151bcd91df","cell_type":"code","source":"# Imprimir el tercer ejemplo\n#third_example = tweets['text'][20]\n#print(third_example)\n#print(f\"{'=' * 50}\")\n# Abrir y leer el archivo de texto\nwith open('../data/poem_wordsworth.txt', 'r', encoding='utf-8') as file:\n    content = file.readlines()\n\n# Ver las primeras lÃ­neas del archivo para verificar su contenido\nfor line in content[:5]:  # Muestra las primeras 5 lÃ­neas\n    print(line)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"I wandered lonely as a cloud\n\n\n\n\n\nI wandered lonely as a cloud\n\nThat floats on high o'er vales and hills,\n\n"}],"execution_count":12},{"id":"853a4b83-f503-4405-aedd-66bbc088e3e7","cell_type":"markdown","source":"Vamos a intentar con otro tweet. Que has notado?","metadata":{}},{"id":"5b3c2f60-fc92-4326-bad6-5ad04be50476","cell_type":"code","source":"# Imprimimos otro tweet\n#print(tweets['text'][100])\n#print(f\"{'=' * 50}\")\n# Leer el archivo como texto \nwith open('../data/poem_wordsworth.txt', 'r', encoding='utf-8') as file:\n    lines = file.readlines()\n\n# Crear un DataFrame con las lÃ­neas leÃ­das\ntweets = pd.DataFrame(lines, columns=['text'])\n\n# Imprimir el primer tweet\nprint(tweets['text'][0])\n\n# Aplicar la funciÃ³n remove_punct() al primer tweet\nimport string\ndef remove_punct(text):\n    return ''.join([char for char in text if char not in string.punctuation])\n\nprint(remove_punct(tweets['text'][0]))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"I wandered lonely as a cloud\n\nI wandered lonely as a cloud\n\n"}],"execution_count":21},{"id":"1af02ce5-b674-4cb4-8e08-7d7416963f9c","cell_type":"markdown","source":"QuÃ© tal el siguiente ejemplo?","metadata":{}},{"id":"6f8c3947-e6b8-42fe-8a58-15e4b6c60005","cell_type":"code","source":"# Imprimimos el texto con contracciÃ³n \ncontraction_text = \"We've got quite a bit of punctuation here, don't we?!? #Python @D-Lab.\"\n\n# Aplicar las funciones\nremove_punct(contraction_text)","metadata":{"trusted":true},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'Weve got quite a bit of punctuation here dont we Python DLab'"},"metadata":{}}],"execution_count":23},{"id":"62574c66-db3f-4500-9c3b-cea2f3eb2a30","cell_type":"markdown","source":"âš ï¸ **Advertencia:** en algÃºn caso, nosotros queremos remover la tokenizaciÃ³n de puntuaciones marcadas **after** , cual discutiriamos en minutos. Esto nos diria que el orden de preprocesamiento **order** es un asunto de importancia!","metadata":{}},{"id":"58c6b85e-58e7-4f56-9b4a-b60c85b394ba","cell_type":"markdown","source":"## ğŸ¥Š Reto 1: Preprocesamiento con multiples pasos \n\nEntonces ahora hemos aprendido algunas operaciones de preprocesamiento. Â¡CombinÃ©moslas en una funciÃ³n! Esta funciÃ³n te resultarÃ¡ Ãºtil si trabajas con datos de texto en inglÃ©s confusos y quieres preprocesarlos con una sola funciÃ³n.\n\nA continuaciÃ³n se muestra el ejemplo de datos de texto para el desafÃ­o 1. Escribe una funciÃ³n para:\n- Convertir el texto en minÃºsculas\n- Eliminar signos de puntuaciÃ³n\n- Eliminar espacios en blanco adicionales\n\nPuedes reciclar el cÃ³digo que usamos anteriormente.!","metadata":{}},{"id":"deb10cba-239e-4856-b56d-7d5eb850c9b9","cell_type":"code","source":"challenge1_path = '../data/example1.txt'\n\nwith open(challenge1_path, 'r') as file:\n    challenge1 = file.read()\n    \nprint(challenge1)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"\n\nThis is a text file that has some extra blankspace at the start and end. Blankspace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines.\n\n\nThe Python method called \"strip\" only catches blankspace at the start and end of a string. But it won't catch it in       the middle,\t\tfor example,\n\nin this sentence.\t\tOnce again, regular expressions will\n\nhelp\t\tus    with this.\n\n\n\n"}],"execution_count":24},{"id":"e2480823-65dd-4f52-a7b3-6d9b10d87912","cell_type":"code","source":"def clean_text(text):\n\n    # Step 1: Lowercase\n    text = ...\n\n    # Step 2: Use remove_punct to remove punctuation marks\n    text = ...\n\n    # Step 3: Remove extra whitespace characters\n    text = ...\n\n    return text","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":25},{"id":"dc603506-0adb-45d7-bb6f-62958c054fdd","cell_type":"code","source":"# Descomentar la aplicaciÃ³n sobre la funciÃ³n del reto 1 \nclean_text(challenge1)","metadata":{"scrolled":true,"trusted":true},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Ellipsis"},"metadata":{}}],"execution_count":26},{"id":"67c159cb-8eaa-4c30-b8ff-38a712d2bb0f","cell_type":"markdown","source":"## Procesos de Tareas especÃ­ficas\n\nAhora que comprendemos las operaciones comunes de preprocesamiento, aÃºn quedan algunas operaciones adicionales por considerar. Nuestros datos de texto podrÃ­an requerir una mayor normalizaciÃ³n segÃºn el idioma, la fuente y el contenido de los datos.\n\nPor ejemplo, si trabajamos con documentos financieros, podrÃ­amos querer estandarizar los sÃ­mbolos monetarios convirtiÃ©ndolos en dÃ­gitos. En nuestros datos de tuits, existen numerosos hashtags y URL. Estos pueden reemplazarse con marcadores de posiciÃ³n para simplificar el anÃ¡lisis posterior.s.","metadata":{}},{"id":"c2936cea-74e9-40c2-bfbe-6ba8129330de","cell_type":"markdown","source":"### ğŸ¬ **Demo**: Eliminar Hashtags y URLs \n\nAunque las URL, los hashtags y los nÃºmeros son informativos por sÃ­ mismos, a menudo no nos importa su significado exacto.\n\nSi bien podrÃ­amos eliminarlos por completo, suele ser informativo saber que existe una URL o un hashtag. En la prÃ¡ctica, reemplazamos las URL y los hashtags individuales con un \"sÃ­mbolo\" que preserva la existencia de estas estructuras en el texto. Lo habitual es usar las cadenas \"URL\" y \"HASHTAG\".\n\nDado que estos tipos de texto suelen seguir una estructura regular, son un ejemplo adecuado para el uso de expresiones regulares. Apliquemos estos patrones a los datos de los tweets.","metadata":{}},{"id":"03c0dc37-a013-4f0a-b72f-a1f64dc6c1bd","cell_type":"code","source":"# Imprimir un ejemplo de tweet \nurl_tweet = tweets['text'][13]\nprint(url_tweet)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Along the margin of a bay:\n\n"}],"execution_count":30},{"id":"4ef61bea-ea11-468d-8176-a2f63659d204","cell_type":"code","source":"# URL \nurl_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\nurl_repl = ' URL '\nre.sub(url_pattern, url_repl, url_tweet)","metadata":{"trusted":true},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"'Along the margin of a bay:\\n'"},"metadata":{}}],"execution_count":28},{"id":"ea8e0f2a-460e-4088-aa89-dc2a8bc6f7fe","cell_type":"code","source":"# Hashtag\nhashtag_pattern = r'(?:^|\\s)[ï¼ƒ#]{1}(\\w+)'\nhashtag_repl = ' HASHTAG '\nre.sub(hashtag_pattern, hashtag_repl, url_tweet)","metadata":{"trusted":true},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'Along the margin of a bay:\\n'"},"metadata":{}}],"execution_count":29},{"id":"d7943ed9-70de-4f4a-b1bb-b2896d05e618","cell_type":"markdown","source":"## Referencias\n\n1. A tutorial introducing the tokenization scheme in BERT: [The huggingface NLP course on wordpiece tokenization](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)\n2. A specific example of \"failure\" in tokenization: [Weaknesses of wordpiece tokenization: Findings from the front lines of NLP at VMware.](https://medium.com/@rickbattle/weaknesses-of-wordpiece-tokenization-eb20e37fec99)\n3. How does BERT decide boundaries between subtokens: [Subword tokenization in BERT](https://tinkerd.net/blog/machine-learning/bert-tokenization/#subword-tokenization)","metadata":{}},{"id":"ce0812a7-f033-46ed-bc7b-67109c369e6c","cell_type":"markdown","source":"<div class=\"alert alert-success\">\n\n## â— Puntos claves \n\n* Preprocesamiento incluido en los ultimos pasos, algunos de estos son mas comunes para datos de textos independientes, y algunas son tareas especificas. \n* Ambas `nltk` y `spaCy` podrÃ­a utilizarse para tokenizar y eliminar palabras vacÃ­as. Esta Ãºltima opciÃ³n es mÃ¡s eficaz para proporcionar diversas anotaciones lingÃ¼Ã­sticas. \n* La tokenizaciÃ³n funciona de manera diferente en BERT, que a menudo implica dividir una palabra completa en subpalabras. \n\n</div>","metadata":{}},{"id":"f4f567b4-8661-4a9d-84c9-4b753732853e","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}