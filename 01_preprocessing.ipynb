{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d3e7ea21-6437-48e8-a9e4-3bdc05f709c9","cell_type":"markdown","source":"# Analisis de texto en Python: Preprocesamiento\n\n* * * \n\n<div class=\"alert alert-success\">  \n    \n### Objetivos de aprendizaje \n    \n* Aprender pasos comunes para preprocesamiento de datos de texto, tan bien como especificar operaciones para preprocesamiento de datos de Twitter.\n* Conocer los paquetes NPL comunmente usado y acoplarlos.\n* Entender tokenizadores, y como cambiar desde la llegada de los grandes modelos de lenguaje.\n</div>\n\n### Icons Used in This Notebook\n🔔 **Pregunta**: Una pregunta rapida para ayudar a entender que esta sucediendo.<br>\n🥊 **Reto**: Ejercicios interactivos. Trabajaremos a traves de esto en el taller!<br>\n⚠️ **Advertencia:** Aviso sobre cosas complicadas o errores comunes.<br>\n🎬 **Demo**: Mostrando algo más avanzado: para que sepas para qué se puede usar Python!<br> \n\n### Secciones\n1. [Preprocessing](#section1)\n2. [Tokenization](#section2)\n\nEn esta seccion de tres parte del taller, aprenderemos la construccion de bloques para ejecutar anlisis de texto en Python. Estas tecnicas se encuentran en el dominio de Procesamiento de Lenguaje Natural (NLP). NLP es un campo que se ocupa con identificacion y extrayendo patrones de lenguaje, ante todo escribiendo textos. A lo largo de la serie de talleres, interactuaremos con paquetes para ejecutar analisis de texto: empezando desde metodos de cadena simple hasta paquetes especificos  NLP, tal como `nltk`, `spaCy`, y mas recientes sobre Modelos de Lenguaje Grande (`BERT`).\n\nAhora, Instalemos estos paquetes correctamente ante de introducirnos en la materia.","metadata":{}},{"id":"d442e4c7-e926-493d-a64e-516616ad915a","cell_type":"code","source":"# Uncomment the following lines to install packages/model\n%pip install NLTK\n%pip install transformers\n%pip install spaCy\n!python -m spacy download en_core_web_sm","metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting NLTK\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting click (from NLTK)\n  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\nCollecting joblib (from NLTK)\n  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting regex>=2021.8.3 (from NLTK)\n  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\nCollecting tqdm (from NLTK)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\nDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\nDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nInstalling collected packages: tqdm, regex, joblib, click, NLTK\nSuccessfully installed NLTK-3.9.1 click-8.1.8 joblib-1.4.2 regex-2024.11.6 tqdm-4.67.1\nNote: you may need to restart the kernel to use updated packages.\nCollecting transformers\n  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\nCollecting filelock (from transformers)\n  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\nCollecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\nCollecting numpy>=1.17 (from transformers)\n  Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\nRequirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /srv/conda/envs/notebook/lib/python3.10/site-packages (from transformers) (2.32.3)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting safetensors>=0.4.3 (from transformers)\n  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nRequirement already satisfied: tqdm>=4.27 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from transformers) (4.67.1)\nCollecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.26.0->transformers)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\nDownloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\nDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\nDownloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\nDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\nDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m135.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\nInstalling collected packages: safetensors, numpy, fsspec, filelock, huggingface-hub, tokenizers, transformers\nSuccessfully installed filelock-3.18.0 fsspec-2025.3.0 huggingface-hub-0.29.3 numpy-2.2.4 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.0\nNote: you may need to restart the kernel to use updated packages.\nCollecting spaCy\n  Downloading spacy-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\nCollecting spacy-legacy<3.1.0,>=3.0.11 (from spaCy)\n  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\nCollecting spacy-loggers<2.0.0,>=1.0.0 (from spaCy)\n  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\nCollecting murmurhash<1.1.0,>=0.28.0 (from spaCy)\n  Downloading murmurhash-1.0.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\nCollecting cymem<2.1.0,>=2.0.2 (from spaCy)\n  Downloading cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\nCollecting preshed<3.1.0,>=3.0.2 (from spaCy)\n  Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\nCollecting thinc<8.4.0,>=8.3.4 (from spaCy)\n  Downloading thinc-8.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\nCollecting wasabi<1.2.0,>=0.9.1 (from spaCy)\n  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\nCollecting srsly<3.0.0,>=2.4.3 (from spaCy)\n  Downloading srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\nCollecting catalogue<2.1.0,>=2.0.6 (from spaCy)\n  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\nCollecting weasel<0.5.0,>=0.1.0 (from spaCy)\n  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\nCollecting typer<1.0.0,>=0.3.0 (from spaCy)\n  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from spaCy) (4.67.1)\nRequirement already satisfied: numpy>=1.19.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from spaCy) (2.2.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from spaCy) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from spaCy) (2.0.3)\nRequirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from spaCy) (3.1.5)\nRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.10/site-packages (from spaCy) (75.8.0)\nRequirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from spaCy) (24.2)\nCollecting langcodes<4.0.0,>=3.2.0 (from spaCy)\n  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\nCollecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spaCy)\n  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.3.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (2.3.0)\nRequirement already satisfied: typing-extensions>=4.6.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (4.12.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2024.12.14)\nCollecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spaCy)\n  Downloading blis-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nCollecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spaCy)\n  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: click>=8.0.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spaCy) (8.1.8)\nCollecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spaCy)\n  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\nCollecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spaCy)\n  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\nCollecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spaCy)\n  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\nCollecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spaCy)\n  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from jinja2->spaCy) (3.0.2)\nCollecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spaCy)\n  Downloading marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\nCollecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy)\n  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (2.19.1)\nCollecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spaCy)\n  Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\nCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy)\n  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\nDownloading spacy-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.2/29.2 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\nDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\nDownloading cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (204 kB)\nDownloading langcodes-3.5.0-py3-none-any.whl (182 kB)\nDownloading murmurhash-1.0.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\nDownloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\nDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\nDownloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\nDownloading srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\nDownloading thinc-8.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m126.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading typer-0.15.2-py3-none-any.whl (45 kB)\nDownloading wasabi-1.1.3-py3-none-any.whl (27 kB)\nDownloading weasel-0.4.1-py3-none-any.whl (50 kB)\nDownloading blis-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\nDownloading confection-0.1.5-py3-none-any.whl (35 kB)\nDownloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m160.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\nDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\nDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\nDownloading marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\nDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\nDownloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\nDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nInstalling collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, blis, srsly, smart-open, preshed, markdown-it-py, language-data, rich, langcodes, confection, typer, thinc, weasel, spaCy\nSuccessfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 preshed-3.0.9 rich-13.9.4 shellingham-1.5.4 smart-open-7.1.0 spaCy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\nNote: you may need to restart the kernel to use updated packages.\nCollecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-3.8.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n"}],"execution_count":1},{"id":"df5b8f8e-4e69-426e-a202-ec48b325e89a","cell_type":"markdown","source":"<a id='section1'></a>\n\n# Preprocesamiento\n\nEn la parte 1 de este taller, Direccionaremos el primer paso de analisis de texto. Nuestra meta es convertir la fila, datos de texto desordenados en un formato consistente. Este proceso es a menudo llamado **preprocesamiento**, **limpieza de texto**, o **normalización de texto**.\n\nNotaras que al final del procesamiento,nuestro sto es aun en formato que nosostros podemos leer y entender. En la parte 2 y 3, comenzaremos nuestra incursión en la conversión de datos de texto en una representación numérica, un formato que las computadoras pueden manejar más fácilmente. \n\n🔔 **Pregunta**: Vamos a pausar un momento para reflexionar sobre **sus** previas experiencias trabajando sobre texto de datos. \n- Cual es el formato de los datos de texto con los que has interactuado (plain text, CSV, or XML)?\n- De donde viene (structured corpus, scraped from the web, survey data)?\n- Esta desordenado (i.e., is the data formatted consistently)?","metadata":{}},{"id":"4b35911a-3b3f-4a48-a7d1-9882aab04851","cell_type":"markdown","source":"## Procesos Comunes\n\nPreprocesamiento no es aldo que podamos lograr con una simple linea de código. Nosotros a menudo empezamos por familiarizarnos nosotros mismo con los datos, y en el camino, obtenemos una comprensión más clara de la granularidad del preprocesamiento que queremos aplicar.\n\nInicialmente, comenzamos aplicando un conjunto de procesos comúnmente utilizados para limpiar los datos. Estas operaciones so alteran facilmente la forma o el significado de los datos; sirven como un procedimiento estandarizado para remodelar los datos en un formato consistente.\n\nLa siguientes procesos, por ejemplo, son comunmente aplicados para procesos de textos en ingles de varios generos. Estas operaciones pueden  estar siendo usadas para funciones integrles en Python, tal como metodos`string`, y expresiones regulares. \n- El texto en minuscula\n- Remover puntuaciones marcadas \n- Remover caracteres de espacio en blanco\n- Remover palabras en stop \n\nDespués al iniciar el procesamiento, nosotros debemos cambiar para realizar procesos de tareas especificas, cuyos detalles a menudo dependen de la tarea posterior que queremos realizar y de la naturaleza de los datos de texto (i.e., its stylistic and linguistic features).  \n\nAntes de adentrarnos en estas operaciones, echemos un vistazo a nuestros datos!\n","metadata":{}},{"id":"ec5d7350-9a1e-4db9-b828-a87fe1676d8d","cell_type":"markdown","source":"### Importar el texto de datos\n\nEl texto de datos, podemos estar trabajando con un archivo CSV. Contiene tuits sobre aerolíneas estadounidenses, eliminados desde febrero de 2015. \n\nVamos a leer el archivo `airline_tweets.csv` dentro del dataframe con `pandas`.","metadata":{}},{"id":"7228677e-001b-4484-b85d-9d4218a1469d","cell_type":"code","source":"!pip show pandas","metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":"\u001b[33mWARNING: Package(s) not found: pandas\u001b[0m\u001b[33m\n\u001b[0m"}],"execution_count":1},{"id":"f912f8a1-662c-4e66-88ad-36460c6db98c","cell_type":"code","source":"!pip install pandas","metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting pandas\n  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\nRequirement already satisfied: numpy>=1.22.4 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2.2.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2025.1)\nCollecting tzdata>=2022.7 (from pandas)\n  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\nInstalling collected packages: tzdata, pandas\nSuccessfully installed pandas-2.2.3 tzdata-2025.2\n"}],"execution_count":2},{"id":"3d1ff64b-53ad-4eca-b846-3fda20085c43","cell_type":"code","source":"# Import pandas\nimport pandas as pd\n\n# File path to data\ncsv_path = '../data/airline_tweets.csv'\n\n# Specify the separator\ntweets = pd.read_csv(csv_path, sep=',')","metadata":{"trusted":false},"outputs":[],"execution_count":3},{"id":"e397ac6a-c2ba-4cce-8700-b36b38026c9d","cell_type":"code","source":"# Show the first five rows\ntweets.head()","metadata":{"trusted":false},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>airline_sentiment</th>\n      <th>airline_sentiment_confidence</th>\n      <th>negativereason</th>\n      <th>negativereason_confidence</th>\n      <th>airline</th>\n      <th>airline_sentiment_gold</th>\n      <th>name</th>\n      <th>negativereason_gold</th>\n      <th>retweet_count</th>\n      <th>text</th>\n      <th>tweet_coord</th>\n      <th>tweet_created</th>\n      <th>tweet_location</th>\n      <th>user_timezone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>570306133677760513</td>\n      <td>neutral</td>\n      <td>1.0000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>cairdin</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica What @dhepburn said.</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:35:52 -0800</td>\n      <td>NaN</td>\n      <td>Eastern Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>570301130888122368</td>\n      <td>positive</td>\n      <td>0.3486</td>\n      <td>NaN</td>\n      <td>0.0000</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica plus you've added commercials t...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:59 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>570301083672813571</td>\n      <td>neutral</td>\n      <td>0.6837</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>yvonnalynn</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:48 -0800</td>\n      <td>Lets Play</td>\n      <td>Central Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>570301031407624196</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Bad Flight</td>\n      <td>0.7033</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica it's really aggressive to blast...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:36 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>570300817074462722</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Can't Tell</td>\n      <td>1.0000</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica and it's a really big bad thing...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:14:45 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n0  570306133677760513           neutral                        1.0000   \n1  570301130888122368          positive                        0.3486   \n2  570301083672813571           neutral                        0.6837   \n3  570301031407624196          negative                        1.0000   \n4  570300817074462722          negative                        1.0000   \n\n  negativereason  negativereason_confidence         airline  \\\n0            NaN                        NaN  Virgin America   \n1            NaN                     0.0000  Virgin America   \n2            NaN                        NaN  Virgin America   \n3     Bad Flight                     0.7033  Virgin America   \n4     Can't Tell                     1.0000  Virgin America   \n\n  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n0                    NaN     cairdin                 NaN              0   \n1                    NaN    jnardino                 NaN              0   \n2                    NaN  yvonnalynn                 NaN              0   \n3                    NaN    jnardino                 NaN              0   \n4                    NaN    jnardino                 NaN              0   \n\n                                                text tweet_coord  \\\n0                @VirginAmerica What @dhepburn said.         NaN   \n1  @VirginAmerica plus you've added commercials t...         NaN   \n2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n3  @VirginAmerica it's really aggressive to blast...         NaN   \n4  @VirginAmerica and it's a really big bad thing...         NaN   \n\n               tweet_created tweet_location               user_timezone  \n0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"execution_count":4},{"id":"ae3b339f-45cf-465d-931c-05f9096fd510","cell_type":"markdown","source":"El marco de datos tiene una fila por tuit. El texto del tuit se muestra en la columna `text`.\n- `text` (`str`): el texto del tweet.\n\nOtros metadatos que nos interesan incluyen: \n- `airline_sentiment` (`str`): the sentiment of the tweet, etiquetado como as \"neutral,\" \"positive,\" o \"negative.\"\n- `airline` (`str`): the airline that is tweeted about.\n- `retweet count` (`int`): como algunos tiempo el tweet fueron retweeteados.","metadata":{}},{"id":"302c695b-4bd1-4151-9cb9-ef5253eb16df","cell_type":"markdown","source":"Echemos un vistazo a algunos de los tweets:","metadata":{}},{"id":"b690daab-7be5-4b8f-8af0-a91fdec4ec4f","cell_type":"code","source":"!pip install pandas\n# Ejemplo de creación de un DataFrame llamado 'tweets'\nimport pandas as pd\ntweets = pd.DataFrame({\n    'text': ['tweet 1', 'tweet 2', 'tweet 3']\n})\nprint(tweets['text'].iloc[0])\nprint(tweets['text'].iloc[1])\nprint(tweets['text'].iloc[2])\nprint(tweets.head())","metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.10/site-packages (2.2.3)\nRequirement already satisfied: numpy>=1.22.4 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2.2.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2025.2)\nRequirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\ntweet 1\ntweet 2\ntweet 3\n      text\n0  tweet 1\n1  tweet 2\n2  tweet 3\n"}],"execution_count":7},{"id":"8adc05fa-ad30-4402-ab56-086bcb09a166","cell_type":"markdown","source":"🔔 **Pregunta**: Que has notado? Cuáles son las características estilísticas de los tweets?","metadata":{}},{"id":"c3460393-00a6-461c-b02a-9e98f9b5d1af","cell_type":"markdown","source":"### Lowercasing\n\nMientras reconocasmos que el uso de mayúsculas y minúsculas de una palabra es informativo, a menudo no trabajamos en contextos en los que podamos utilizar adecuadamente esta información.\n\nMas a menudo, el análisis posterior nosotros realizamos **case-insensitive**. Por ejemplo, en el análisis de frecuencia, queremos tener en cuenta las diversas formas de una misma palabra. Convertir los datos de texto en minúsculas facilita este proceso y simplifica nuestro análisis.\n\nPodemos lograr fácilmente la conversión a minúsculas con el método de cadena [`.lower()`](https://docs.python.org/3/library/stdtypes.html#str.lower); see [documentation](https://docs.python.org/3/library/stdtypes.html#string-methods) para mas funciones útiles.\n\nVamos aplicar esto en el sigiente ejemplo:","metadata":{}},{"id":"58a95d90-3ef1-4bff-9cfe-d447ed99f252","cell_type":"code","source":"# Verificar cuántas filas tiene el DataFrame / se cambia el código ya que no se define bien \nprint(\"Número de filas en el DataFrame:\", tweets.shape[0])\n\n# Verificar si el índice 108 está dentro del rango de las filas\nif tweets.shape[0] > 108:\n    first_example = tweets['text'].iloc[108]  # Accede al tweet en el índice 108\n    print(first_example)\nelse:\n    print(\"Índice 108 fuera de rango. El DataFrame solo tiene\", tweets.shape[0], \"filas.\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Número de filas en el DataFrame: 3\nÍndice 108 fuera de rango. El DataFrame solo tiene 3 filas.\n"}],"execution_count":17},{"id":"c66d91c0-6eed-4591-95fc-cd2eae2e0d41","cell_type":"code","source":"# Ejemplo: asignar un valor a 'first_example'\nfirst_example = \"Este es un ejemplo de texto\"\n\n# Dado que el codigo anterior no ejecuta ahora el código esta sin errores\n\n# Revisar si todos los caracteres están en minúscula \nprint(first_example.islower())  # Devuelve True si todos los caracteres son minúsculas\nprint(f\"{'=' * 50}\")\n\n# Convertirlo a minúsculas\nprint(first_example.lower())  # Convierte a minúsculas\nprint(f\"{'=' * 50}\")\n\n# Convertirlo a mayúsculas\nprint(first_example.upper())  # Convierte a mayúsculas","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"False\n==================================================\neste es un ejemplo de texto\n==================================================\nESTE ES UN EJEMPLO DE TEXTO\n"}],"execution_count":20},{"id":"7bf0d8c8-bd6c-47ef-b305-09ac61d07d4d","cell_type":"markdown","source":"### Eliminar caracteres de espacio en blanco adicionales\n\nA veces nosotros podriamos encontrar textos con espacios en blanco extra, such as spaces, tabs, and newline characters, which is particularly common when the text is scrapped from web pages. Before we dive into the details, let's briefly introduce Regular Expressions (regex) and the `re` package. \n\nLas expresiones regulares son una forma eficaz de buscar patrones de cadenas específicos en corpus grandes. Su curva de aprendizaje es notablemente pronunciada, pero pueden ser muy eficientes una vez que las dominamos. Muchos paquetes de PNL dependen en gran medida de las expresiones regulares internamente. Los evaluadores de expresiones regulares, como [regex101](https://regex101.com), son herramientas utiles are useful tools tanto en la comprensión como en la creación de expresiones regulares.\n\nNuestro objetivo en este taller no es proporcionar una inmersión profunda (ni siquiera superficial) en las expresiones regulares; en cambio, queremos exponerlo a ellas para que esté mejor preparado para realizar inmersiones profundas en el futuro.\n\nEl siguiente ejemplo es un poema de William Wordsworth. Como muchos poemas, el texto puede contener saltos de línea adicionales. (i.e., newline characters, `\\n`) que queremos eliminar.","metadata":{}},{"id":"d1bd73f1-a30f-4269-a05e-47cfff7b496f","cell_type":"code","source":"# File path to the poem\ntext_path = '../data/poem_wordsworth.txt'\n\n# Read the poem in\nwith open(text_path, 'r') as file:\n    text = file.read()\n    file.close()","metadata":{"trusted":true},"outputs":[],"execution_count":1},{"id":"7a693dd9-9706-40b3-863f-f568020245f7","cell_type":"markdown","source":"Como puedes ver, el poema es fomateado como ua cedena continua de textos con los saltos de línea se colocan al final de cada línea, lo que dificulta la lectura. ","metadata":{}},{"id":"7e78a75a-8e15-4bcb-a416-783aa7f60ef3","cell_type":"code","source":"text","metadata":{"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"\"I wandered lonely as a cloud\\n\\n\\nI wandered lonely as a cloud\\nThat floats on high o'er vales and hills,\\nWhen all at once I saw a crowd,\\nA host, of golden daffodils;\\nBeside the lake, beneath the trees,\\nFluttering and dancing in the breeze.\\n\\nContinuous as the stars that shine\\nAnd twinkle on the milky way,\\nThey stretched in never-ending line\\nAlong the margin of a bay:\\nTen thousand saw I at a glance,\\nTossing their heads in sprightly dance.\\n\\nThe waves beside them danced; but they\\nOut-did the sparkling waves in glee:\\nA poet could not but be gay,\\nIn such a jocund company:\\nI gazed—and gazed—but little thought\\nWhat wealth the show to me had brought:\\n\\nFor oft, when on my couch I lie\\nIn vacant or in pensive mood,\\nThey flash upon that inward eye\\nWhich is the bliss of solitude;\\nAnd then my heart with pleasure fills,\\nAnd dances with the daffodils.\""},"metadata":{}}],"execution_count":3},{"id":"47cce993-c315-4aaa-87fe-149de8607f65","cell_type":"markdown","source":"Una función útil que podemos utilizar para mostrar el poema correctamente es `.splitlines()`. Como sugiere el nombre, divide una secuencia de texto larga en una lista de líneas siempre que haya un carácter de nueva línea.   ","metadata":{}},{"id":"ddeade7a-065d-49e6-bdd3-87a8ea8f6e6e","cell_type":"code","source":"# Dividir la cadena única en una lista de líneas\ntext.splitlines()","metadata":{"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['I wandered lonely as a cloud',\n '',\n '',\n 'I wandered lonely as a cloud',\n \"That floats on high o'er vales and hills,\",\n 'When all at once I saw a crowd,',\n 'A host, of golden daffodils;',\n 'Beside the lake, beneath the trees,',\n 'Fluttering and dancing in the breeze.',\n '',\n 'Continuous as the stars that shine',\n 'And twinkle on the milky way,',\n 'They stretched in never-ending line',\n 'Along the margin of a bay:',\n 'Ten thousand saw I at a glance,',\n 'Tossing their heads in sprightly dance.',\n '',\n 'The waves beside them danced; but they',\n 'Out-did the sparkling waves in glee:',\n 'A poet could not but be gay,',\n 'In such a jocund company:',\n 'I gazed—and gazed—but little thought',\n 'What wealth the show to me had brought:',\n '',\n 'For oft, when on my couch I lie',\n 'In vacant or in pensive mood,',\n 'They flash upon that inward eye',\n 'Which is the bliss of solitude;',\n 'And then my heart with pleasure fills,',\n 'And dances with the daffodils.']"},"metadata":{}}],"execution_count":5},{"id":"44d3825b-0857-44e1-bf6a-d8c7a9032704","cell_type":"markdown","source":"Vamos a retornar a nuestros datos tweet para un ejemplo.","metadata":{}},{"id":"53a81ea9-65c4-474a-8530-35393555d1be","cell_type":"code","source":"# Imprimir el segundo ejemplo \n#second_example = tweets['text'][5]\n#second_example\n!pip install pandas\nimport pandas as pd\nimport os\n\nimport pandas as pd\nimport os\n\n# Verificar el directorio actual de trabajo\nprint(f\"Directorio actual de trabajo: {os.getcwd()}\")\n\n# Leer el archivo de texto como líneas simples\nfile_path = '../data/poem_wordsworth.txt'  \n\n# Leer el archivo línea por línea\nwith open(file_path, 'r') as file:\n    lines = file.readlines()\n\n# Imprimir las primeras 5 líneas del archivo para verificar su contenido\nprint(\"Primeras 5 líneas del archivo:\")\nfor i in range(min(5, len(lines))):  # Limitar a 5 líneas\n    print(lines[i])\n\ntweets = pd.DataFrame(lines, columns=['text'])\n\n# Verificar el contenido cargado\nprint(\"\\nPrimeras 5 filas del DataFrame:\")\nprint(tweets.head())\n\n# Ahora, si quieres acceder a un ejemplo específico del DataFrame:\nsecond_example = tweets['text'].iloc[5]  # Acceder al segundo ejemplo\nprint(\"\\nSegundo ejemplo del DataFrame:\")\nprint(second_example)\n\n# Si deseas realizar otras operaciones en los datos, puedes hacerlo ahora, por ejemplo, convertir el texto a minúsculas:\nprint(\"\\nTexto en minúsculas del segundo ejemplo:\")\nprint(second_example.lower())","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.10/site-packages (2.2.3)\nRequirement already satisfied: numpy>=1.22.4 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2.2.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pandas) (2025.2)\nRequirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nDirectorio actual de trabajo: /home/jovyan/lessons\nPrimeras 5 líneas del archivo:\nI wandered lonely as a cloud\n\n\n\n\n\nI wandered lonely as a cloud\n\nThat floats on high o'er vales and hills,\n\n\nPrimeras 5 filas del DataFrame:\n                                          text\n0               I wandered lonely as a cloud\\n\n1                                           \\n\n2                                           \\n\n3               I wandered lonely as a cloud\\n\n4  That floats on high o'er vales and hills,\\n\n\nSegundo ejemplo del DataFrame:\nWhen all at once I saw a crowd,\n\n\nTexto en minúsculas del segundo ejemplo:\nwhen all at once i saw a crowd,\n\n"}],"execution_count":16},{"id":"aef55865-36fd-4c06-a765-530cf3b53096","cell_type":"markdown","source":"En este caso, no queremos dividir el tweet en una lista de cadenas. Seguimos esperando una sola cadena de texto, pero queremos eliminar por completo el salto de línea.\n\nEl método string `.strip()` elimina eficazmente los espacios en ambos extremos del texto. Sin embargo, no funcionará en nuestro ejemplo, ya que el carácter de nueva línea está en el medio de el string.","metadata":{}},{"id":"b933503b-4370-4dc4-b287-6dc2f9cdb1d4","cell_type":"code","source":"# Solo elimina los espacios en blanco en ambos extremos\nsecond_example.strip()","metadata":{"trusted":true},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'When all at once I saw a crowd,'"},"metadata":{}}],"execution_count":17},{"id":"b99b80b4-804f-460f-a2d5-adbd654902b3","cell_type":"markdown","source":"Este es donde la nube regex es realmete útil.","metadata":{}},{"id":"ceac9714-7053-4b2e-affb-71f8c3d2dcd9","cell_type":"code","source":"import re","metadata":{"trusted":true},"outputs":[],"execution_count":18},{"id":"d5f08d20-ba81-4e48-9e2a-5728148005b3","cell_type":"markdown","source":"Ahora, con regex, básicamente, la llamamos para que coincida con un patrón identificado en los datos de texto, y queremos realizar algunas operaciones con la parte coincidente: extraerla, reemplazarla con otra cosa o eliminarla por completo. Por lo tanto, el funcionamiento de las expresiones regulares se puede resumir en los siguientes pasos:\n\n- Identificar y escribir los patrones en regex (`r'PATTERN'`)\n- Escribir el remplazo de los patrones(`'REPLACEMENT'`)\n- Llamar la función específica regex  (e.g., `re.sub()`)\n\nEn nuestro ejemplo, el patron que estamos buscando es `\\s`, cuál es el nombre corto en expresión regular para cualquier carácter de espacio en blanco (`\\n` and `\\t` included). También añadimos un cuantificador `+` en el final: `\\s+`. Significa que nos gustaría capturar una o más ocurrencias del carácter de espacio en blanco.","metadata":{}},{"id":"1248d227-1149-4014-94a5-c05592a27a7e","cell_type":"code","source":"# Escribir un patron en regex\nblankspace_pattern = r'\\s+'","metadata":{"trusted":true},"outputs":[],"execution_count":1},{"id":"cc075c2e-1a1d-4393-a3ea-8ad7c118364b","cell_type":"markdown","source":"El reemplazo de uno o más espacios en blanco es exactamente un solo espacio, que es el límite canónico de palabras en inglés. Cualquier espacio adicional se reducirá a un solo espacio. ","metadata":{}},{"id":"c55cb2f1-f4ca-4b79-900c-f65ec303ddac","cell_type":"code","source":"# Ecribir un remplazo para identificación de patrones \nblankspace_repl = ' '","metadata":{"trusted":true},"outputs":[],"execution_count":2},{"id":"bc12e3d1-728a-429b-9c83-4dcc88590bc4","cell_type":"markdown","source":"Por último, pongamos todo junto usando la función [`re.sub()`](https://docs.python.org/3.11/library/re.html#re.sub), Lo que significa que queremos sustituir un patrón por un reemplazo. La función acepta tres argumentos: el patrón, el reemplazo y la cadena a la que queremos aplicar la función.","metadata":{}},{"id":"5249b24b-7111-4569-be29-c40efa5e148e","cell_type":"code","source":"# Remplazar los espacios en blanco(s) con ' '\n#clean_text = re.sub(pattern = blankspace_pattern, \n                    #repl = blankspace_repl, \n                    #string = second_example)\nimport re  # Asegúrate de importar el módulo 're'\n\n# Definir el patrón para los espacios en blanco y el reemplazo\nblankspace_pattern = r'\\s+'  # Esto busca uno o más espacios en blanco\nblankspace_repl = ' '  # Esto reemplaza con un solo espacio\n\n# Suponiendo que 'second_example' es el texto que quieres limpiar\nsecond_example = \"Este es    un  ejemplo   con    espacios   extras.\"\n\n# Remplazar los espacios en blanco(s) con ' '\nclean_text = re.sub(pattern=blankspace_pattern, \n                    repl=blankspace_repl, \n                    string=second_example)\n\nprint(clean_text)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Este es un ejemplo con espacios extras.\n"}],"execution_count":5},{"id":"a895fbe3-a034-4124-94af-72a528913c51","cell_type":"markdown","source":"Ta-da! El carácter de nueva línea ya no está allí.","metadata":{}},{"id":"7087dc0c-5fef-4f1c-8662-7cbc8a978f34","cell_type":"markdown","source":"### Eliminar puntuaciones marcadas \n\nA veces sólo nos interesa analizar **alphanumeric characters** (i.e., the letters and numbers), en tal caso podríamos querer eliminar los signos de puntuación. \n\nEl modulo `string` contiene una lista predefinida de puntuaciones marcadas predefinidas. Vamos a imprimir esto.","metadata":{}},{"id":"70e8502b-b703-45e0-8852-0c3210363440","cell_type":"code","source":"# Cargar una lista predefinida de signos de puntuación\nfrom string import punctuation\nprint(punctuation)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"}],"execution_count":6},{"id":"91119c9e-431c-42cb-afea-f7e607698929","cell_type":"markdown","source":"En la práctica, para eliminar estos caracteres de puntuación, podemos simplemente iterar sobre el texto y eliminar los caracteres que se encuentran en la lista, como se muestra una función a continuación `remove_punct`.","metadata":{}},{"id":"237d868d-339d-4bbe-9a3b-20fa5fbdf231","cell_type":"code","source":"def remove_punct(text):\n    '''Remove punctuation marks in input text'''\n    \n    # Select characters not in puncutaion\n    no_punct = []\n    for char in text:\n        if char not in punctuation:\n            no_punct.append(char)\n\n    # Join the characters into a string\n    text_no_punct = ''.join(no_punct)   \n    \n    return text_no_punct","metadata":{"trusted":true},"outputs":[],"execution_count":7},{"id":"d4fc768b-c2dd-4386-8212-483c4485e4be","cell_type":"markdown","source":"Vamos aplicar la función del ejemplo below. ","metadata":{}},{"id":"7596c465-3d85-4b72-a853-f2151bcd91df","cell_type":"code","source":"# Imprimir el tercer ejemplo\n#third_example = tweets['text'][20]\n#print(third_example)\n#print(f\"{'=' * 50}\")\n# Abrir y leer el archivo de texto\nwith open('../data/poem_wordsworth.txt', 'r', encoding='utf-8') as file:\n    content = file.readlines()\n\n# Ver las primeras líneas del archivo para verificar su contenido\nfor line in content[:5]:  # Muestra las primeras 5 líneas\n    print(line)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"I wandered lonely as a cloud\n\n\n\n\n\nI wandered lonely as a cloud\n\nThat floats on high o'er vales and hills,\n\n"}],"execution_count":12},{"id":"853a4b83-f503-4405-aedd-66bbc088e3e7","cell_type":"markdown","source":"Vamos a intentar con otro tweet. Que has notado?","metadata":{}},{"id":"5b3c2f60-fc92-4326-bad6-5ad04be50476","cell_type":"code","source":"# Imprimimos otro tweet\n#print(tweets['text'][100])\n#print(f\"{'=' * 50}\")\n# Leer el archivo como texto \nwith open('../data/poem_wordsworth.txt', 'r', encoding='utf-8') as file:\n    lines = file.readlines()\n\n# Crear un DataFrame con las líneas leídas\ntweets = pd.DataFrame(lines, columns=['text'])\n\n# Imprimir el primer tweet\nprint(tweets['text'][0])\n\n# Aplicar la función remove_punct() al primer tweet\nimport string\ndef remove_punct(text):\n    return ''.join([char for char in text if char not in string.punctuation])\n\nprint(remove_punct(tweets['text'][0]))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"I wandered lonely as a cloud\n\nI wandered lonely as a cloud\n\n"}],"execution_count":21},{"id":"1af02ce5-b674-4cb4-8e08-7d7416963f9c","cell_type":"markdown","source":"Qué tal el siguiente ejemplo?","metadata":{}},{"id":"6f8c3947-e6b8-42fe-8a58-15e4b6c60005","cell_type":"code","source":"# Imprimimos el texto con contracción \ncontraction_text = \"We've got quite a bit of punctuation here, don't we?!? #Python @D-Lab.\"\n\n# Aplicar las funciones\nremove_punct(contraction_text)","metadata":{"trusted":true},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'Weve got quite a bit of punctuation here dont we Python DLab'"},"metadata":{}}],"execution_count":23},{"id":"62574c66-db3f-4500-9c3b-cea2f3eb2a30","cell_type":"markdown","source":"⚠️ **Advertencia:** en algún caso, nosotros queremos remover la tokenización de puntuaciones marcadas **after** , cual discutiriamos en minutos. Esto nos diria que el orden de preprocesamiento **order** es un asunto de importancia!","metadata":{}},{"id":"58c6b85e-58e7-4f56-9b4a-b60c85b394ba","cell_type":"markdown","source":"## 🥊 Reto 1: Preprocesamiento con multiples pasos \n\nEntonces ahora hemos aprendido algunas operaciones de preprocesamiento. ¡Combinémoslas en una función! Esta función te resultará útil si trabajas con datos de texto en inglés confusos y quieres preprocesarlos con una sola función.\n\nA continuación se muestra el ejemplo de datos de texto para el desafío 1. Escribe una función para:\n- Convertir el texto en minúsculas\n- Eliminar signos de puntuación\n- Eliminar espacios en blanco adicionales\n\nPuedes reciclar el código que usamos anteriormente.!","metadata":{}},{"id":"deb10cba-239e-4856-b56d-7d5eb850c9b9","cell_type":"code","source":"challenge1_path = '../data/example1.txt'\n\nwith open(challenge1_path, 'r') as file:\n    challenge1 = file.read()\n    \nprint(challenge1)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"\n\nThis is a text file that has some extra blankspace at the start and end. Blankspace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines.\n\n\nThe Python method called \"strip\" only catches blankspace at the start and end of a string. But it won't catch it in       the middle,\t\tfor example,\n\nin this sentence.\t\tOnce again, regular expressions will\n\nhelp\t\tus    with this.\n\n\n\n"}],"execution_count":24},{"id":"e2480823-65dd-4f52-a7b3-6d9b10d87912","cell_type":"code","source":"def clean_text(text):\n\n    # Step 1: Lowercase\n    text = ...\n\n    # Step 2: Use remove_punct to remove punctuation marks\n    text = ...\n\n    # Step 3: Remove extra whitespace characters\n    text = ...\n\n    return text","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":25},{"id":"dc603506-0adb-45d7-bb6f-62958c054fdd","cell_type":"code","source":"# Descomentar la aplicación sobre la función del reto 1 \nclean_text(challenge1)","metadata":{"scrolled":true,"trusted":true},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Ellipsis"},"metadata":{}}],"execution_count":26},{"id":"67c159cb-8eaa-4c30-b8ff-38a712d2bb0f","cell_type":"markdown","source":"## Procesos de Tareas específicas\n\nAhora que comprendemos las operaciones comunes de preprocesamiento, aún quedan algunas operaciones adicionales por considerar. Nuestros datos de texto podrían requerir una mayor normalización según el idioma, la fuente y el contenido de los datos.\n\nPor ejemplo, si trabajamos con documentos financieros, podríamos querer estandarizar los símbolos monetarios convirtiéndolos en dígitos. En nuestros datos de tuits, existen numerosos hashtags y URL. Estos pueden reemplazarse con marcadores de posición para simplificar el análisis posterior.s.","metadata":{}},{"id":"c2936cea-74e9-40c2-bfbe-6ba8129330de","cell_type":"markdown","source":"### 🎬 **Demo**: Eliminar Hashtags y URLs \n\nAunque las URL, los hashtags y los números son informativos por sí mismos, a menudo no nos importa su significado exacto.\n\nSi bien podríamos eliminarlos por completo, suele ser informativo saber que existe una URL o un hashtag. En la práctica, reemplazamos las URL y los hashtags individuales con un \"símbolo\" que preserva la existencia de estas estructuras en el texto. Lo habitual es usar las cadenas \"URL\" y \"HASHTAG\".\n\nDado que estos tipos de texto suelen seguir una estructura regular, son un ejemplo adecuado para el uso de expresiones regulares. Apliquemos estos patrones a los datos de los tweets.","metadata":{}},{"id":"03c0dc37-a013-4f0a-b72f-a1f64dc6c1bd","cell_type":"code","source":"# Imprimir un ejemplo de tweet \nurl_tweet = tweets['text'][13]\nprint(url_tweet)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Along the margin of a bay:\n\n"}],"execution_count":30},{"id":"4ef61bea-ea11-468d-8176-a2f63659d204","cell_type":"code","source":"# URL \nurl_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\nurl_repl = ' URL '\nre.sub(url_pattern, url_repl, url_tweet)","metadata":{"trusted":true},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"'Along the margin of a bay:\\n'"},"metadata":{}}],"execution_count":28},{"id":"ea8e0f2a-460e-4088-aa89-dc2a8bc6f7fe","cell_type":"code","source":"# Hashtag\nhashtag_pattern = r'(?:^|\\s)[＃#]{1}(\\w+)'\nhashtag_repl = ' HASHTAG '\nre.sub(hashtag_pattern, hashtag_repl, url_tweet)","metadata":{"trusted":true},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'Along the margin of a bay:\\n'"},"metadata":{}}],"execution_count":29},{"id":"d7943ed9-70de-4f4a-b1bb-b2896d05e618","cell_type":"markdown","source":"## Referencias\n\n1. A tutorial introducing the tokenization scheme in BERT: [The huggingface NLP course on wordpiece tokenization](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)\n2. A specific example of \"failure\" in tokenization: [Weaknesses of wordpiece tokenization: Findings from the front lines of NLP at VMware.](https://medium.com/@rickbattle/weaknesses-of-wordpiece-tokenization-eb20e37fec99)\n3. How does BERT decide boundaries between subtokens: [Subword tokenization in BERT](https://tinkerd.net/blog/machine-learning/bert-tokenization/#subword-tokenization)","metadata":{}},{"id":"ce0812a7-f033-46ed-bc7b-67109c369e6c","cell_type":"markdown","source":"<div class=\"alert alert-success\">\n\n## ❗ Puntos claves \n\n* Preprocesamiento incluido en los ultimos pasos, algunos de estos son mas comunes para datos de textos independientes, y algunas son tareas especificas. \n* Ambas `nltk` y `spaCy` podría utilizarse para tokenizar y eliminar palabras vacías. Esta última opción es más eficaz para proporcionar diversas anotaciones lingüísticas. \n* La tokenización funciona de manera diferente en BERT, que a menudo implica dividir una palabra completa en subpalabras. \n\n</div>","metadata":{}},{"id":"f4f567b4-8661-4a9d-84c9-4b753732853e","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}